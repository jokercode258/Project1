"""
MAIN ENTRY POINT
"""

import argparse
import torch
from pathlib import Path

# Import modules
from train import full_training_pipeline, ChessTrainer
from value_network import ValueNetwork
from minimax_engine import MinimaxEngine
from self_play import SelfPlayManager
from gui import create_gui_with_engine, ChessGUI
import chess


def train_command(args):
    """Train mode"""
    print("=" * 60)
    print("TRAINING AI CHESS")
    print("=" * 60 + "\n")
    
    network, trainer = full_training_pipeline(output_dir=args.output_dir)
    print("\n‚úÖ Training ho√†n th√†nh!")


def play_command(args):
    """Play mode"""
    print("=" * 60)
    print("PLAY CHESS vs AI")
    print("=" * 60 + "\n")
    
    # Color c·ªßa ng∆∞·ªùi ch∆°i
    player_color = chess.WHITE if args.player_color == 'white' else chess.BLACK
    ai_color = chess.BLACK if player_color == chess.WHITE else chess.WHITE
    
    print(f"Ng∆∞·ªùi ch∆°i: {'Tr·∫Øng' if player_color == chess.WHITE else 'ƒêen'}")
    print(f"AI: {'Tr·∫Øng' if ai_color == chess.WHITE else 'ƒêen'}")
    print(f"ƒê·ªô s√¢u Minimax: {args.depth}\n")
    
    # T·∫°o GUI
    gui = create_gui_with_engine(
        model_path=args.model,
        ai_color=ai_color,
        max_depth=args.depth
    )
    
    print("B·∫Øt ƒë·∫ßu game...")
    gui.run()


def selfplay_command(args):
    """Self-play mode"""
    print("=" * 60)
    print("SELF-PLAY")
    print("=" * 60 + "\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load network n·∫øu c√≥ model
    network = None
    if args.model and Path(args.model).exists():
        network = ValueNetwork(hidden_size=128)
        network.load_state_dict(torch.load(args.model, map_location=device))
        network.to(device)
        print(f"ƒê√£ load model: {args.model}\n")
    
    # Self-play manager
    manager = SelfPlayManager(network=network, device=device, minimax_depth=args.depth)
    
    # Ch∆°i games
    white_mode = args.white_mode
    black_mode = args.black_mode
    
    print(f"White: {white_mode}")
    print(f"Black: {black_mode}")
    print(f"S·ªë game: {args.num_games}")
    print(f"Max moves: {args.max_moves}\n")
    
    stats = manager.play_games(
        num_games=args.num_games,
        white_mode=white_mode,
        black_mode=black_mode,
        max_moves=args.max_moves
    )
    
    print(f"\nüìä Stats:")
    print(f"  Tr·∫Øng th·∫Øng: {stats['white_wins']}")
    print(f"  ƒêen th·∫Øng: {stats['black_wins']}")
    print(f"  H√≤a: {stats['draws']}")
    print(f"  Win rate Tr·∫Øng: {stats['white_winrate']:.2%}")
    print(f"  Training samples: {stats['training_samples']}")
    
    # L∆∞u training data
    if args.save_data:
        manager.save_training_data(args.save_data)
        print(f"‚úÖ L∆∞u training data: {args.save_data}")


def analyze_command(args):
    """Analyze mode"""
    print("=" * 60)
    print("ANALYZE POSITION")
    print("=" * 60 + "\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load network
    network = ValueNetwork(hidden_size=128)
    if args.model and Path(args.model).exists():
        network.load_state_dict(torch.load(args.model, map_location=device))
    network.to(device)
    
    # T·∫°o engine
    engine = MinimaxEngine(network, device=device, max_depth=args.depth)
    
    # T·∫°o board
    board = chess.Board(args.fen) if args.fen else chess.Board()
    
    print(f"FEN: {board.fen()}\n")
    
    # Ph√¢n t√≠ch
    move, score = engine.get_best_move_with_score(board)
    
    print(f"N∆∞·ªõc ƒëi t·ªët nh·∫•t: {move}")
    print(f"ƒêi·ªÉm s·ªë: {score:.4f}")
    print(f"S·ªë node ƒë√°nh gi√°: {engine.nodes_evaluated}")
    
    # Top 3 moves
    print(f"\nTop 3 n∆∞·ªõc ƒëi:")
    moves_scores = []
    for move in list(board.legal_moves)[:10]:
        board.push(move)
        _, score = engine.minimax(board, args.depth - 1, 
                                 maximizing=not board.turn)
        board.pop()
        moves_scores.append((move, score))
    
    moves_scores.sort(key=lambda x: x[1], reverse=True)
    for i, (move, score) in enumerate(moves_scores[:3], 1):
        print(f"  {i}. {move} (score={score:.4f})")


def main():
    parser = argparse.ArgumentParser(
        description='AI Chess: Minimax + Neural Network + Self-play + GUI'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Train command
    train_parser = subparsers.add_parser('train', help='Train the model')
    train_parser.add_argument('--output-dir', default='./models',
                            help='Output directory for models')
    train_parser.set_defaults(func=train_command)
    
    # Play command
    play_parser = subparsers.add_parser('play', help='Play vs AI')
    play_parser.add_argument('--model', default='./models/chess_value_network.pth',
                           help='Path to model weights')
    play_parser.add_argument('--depth', type=int, default=3,
                           help='Minimax depth')
    play_parser.add_argument('--player-color', default='white',
                           choices=['white', 'black'],
                           help='Player color')
    play_parser.set_defaults(func=play_command)
    
    # Self-play command
    selfplay_parser = subparsers.add_parser('selfplay', help='Self-play games')
    selfplay_parser.add_argument('--num-games', type=int, default=20,
                               help='Number of games')
    selfplay_parser.add_argument('--white-mode', default='random',
                               choices=['random', 'minimax'],
                               help='White engine mode')
    selfplay_parser.add_argument('--black-mode', default='random',
                               choices=['random', 'minimax'],
                               help='Black engine mode')
    selfplay_parser.add_argument('--max-moves', type=int, default=100,
                               help='Max moves per game')
    selfplay_parser.add_argument('--model', default=None,
                               help='Path to model weights')
    selfplay_parser.add_argument('--depth', type=int, default=3,
                               help='Minimax depth')
    selfplay_parser.add_argument('--save-data', default=None,
                               help='Save training data to file')
    selfplay_parser.set_defaults(func=selfplay_command)
    
    # Analyze command
    analyze_parser = subparsers.add_parser('analyze', help='Analyze position')
    analyze_parser.add_argument('--model', default='./models/chess_value_network.pth',
                              help='Path to model weights')
    analyze_parser.add_argument('--depth', type=int, default=3,
                              help='Minimax depth')
    analyze_parser.add_argument('--fen', default=None,
                              help='FEN string (default: starting position)')
    analyze_parser.set_defaults(func=analyze_command)
    
    args = parser.parse_args()
    
    if args.command:
        args.func(args)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
