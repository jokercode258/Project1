# AI Chess: Minimax + Neural Network + Self-play + GUI

Há»‡ thá»‘ng AI chÆ¡i cá» tÃ­ch há»£p Ä‘áº§y Ä‘á»§ vá»›i Minimax, Neural Network, Self-play, vÃ  GUI Pygame.

## ğŸ“‹ Cáº¥u trÃºc dá»± Ã¡n

project1/
â”œâ”€â”€ board_state.py          # BÆ¯á»šC 1: Biá»ƒu diá»…n tráº¡ng thÃ¡i (12 planes)
â”œâ”€â”€ value_network.py        # BÆ¯á»šC 3: Neural Network (FC layers)
â”œâ”€â”€ minimax_engine.py       # BÆ¯á»šC 5: Minimax + Alpha-Beta + NN
â”œâ”€â”€ self_play.py            # BÆ¯á»šC 2: Self-play Ä‘á»ƒ táº¡o training data
â”œâ”€â”€ train.py                # BÆ¯á»šC 4: Training loop (PyTorch)
â”œâ”€â”€ gui.py                  # BÆ¯á»šC 6: Giao diá»‡n Pygame
â”œâ”€â”€ main.py                 # Entry point chÃ­nh
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md
```

## ğŸ¯ 6 BÆ°á»›c chÃ­nh

### BÆ¯á»šC 1: State Representation (board_state.py)
- Chuyá»ƒn bÃ n cá» â†’ tensor 12Ã—8Ã—8
- 12 planes: 6 quÃ¢n tráº¯ng + 6 quÃ¢n Ä‘en
- Mapping tá»a Ä‘á»™: a8â†’(0,0), h1â†’(7,7)

### BÆ¯á»šC 2: Self-Play (self_play.py)
- AI tá»± chÆ¡i vá»›i chÃ­nh nÃ³
- Ghi láº¡i má»i state + káº¿t quáº£
- Táº¡o training dataset

### BÆ¯á»šC 3: Neural Network (value_network.py)
- Input: 768 (12Ã—8Ã—8)
- Hidden: 128 â†’ 64
- Output: 1 (value âˆˆ [-1, 1])
- Activation: ReLU (hidden), Tanh (output)

### BÆ¯á»šC 4: Training (train.py)
- Loss function: MSELoss
- Optimizer: Adam
- Training loop with validation
- Early stopping + checkpointing

### BÆ¯á»šC 5: Minimax + NN (minimax_engine.py)
- Minimax duyá»‡t cÃ¢y
- Alpha-Beta pruning Ä‘á»ƒ tá»‘i Æ°u
- NN Ä‘Ã¡nh giÃ¡ node lÃ¡
- KhÃ´ng sinh nÆ°á»›c Ä‘i (Minimax chá»§ trÆ°Æ¡ng)

### BÆ¯á»šC 6: GUI (gui.py)
- Váº½ bÃ n cá» 8Ã—8 vá»›i Pygame
- Click Ä‘á»ƒ chá»n quÃ¢n
- Highlight nÆ°á»›c Ä‘i há»£p lá»‡
- AI tá»± Ä‘á»™ng Ä‘i

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Train model

```bash
python main.py train --output-dir ./models
```

Sáº½:
1. ChÆ¡i 20 self-play games
2. Táº¡o training data
3. Train network 100 epochs
4. LÆ°u model tá»‘t nháº¥t

### 3. ChÆ¡i game

```bash
python main.py play --model ./models/chess_value_network.pth --depth 3 --player-color white
```

Äiá»u khiá»ƒn:
- **Click trÃ¡i**: Chá»n quÃ¢n
- **Click pháº£i**: Undo nÆ°á»›c Ä‘i
- **R**: Reset game
- **Q**: Quit

### 4. Self-play (generate more data)

```bash
python main.py selfplay --num-games 50 --white-mode minimax --black-mode random --save-data training_data.npz
```

### 5. Analyze position

```bash
python main.py analyze --model ./models/chess_value_network.pth --depth 3 --fen "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
```

## ğŸ“Š Kiáº¿n trÃºc tá»•ng quÃ¡t

```
NgÆ°á»i chÆ¡i
   â†“
Pygame GUI (gui.py)
   â†“
Chess Board (chess.Board)
   â†“
Minimax Engine (minimax_engine.py)
   â”œâ”€ Duyá»‡t cÃ¢y (Minimax)
   â”œâ”€ Alpha-Beta Pruning
   â””â”€ Evaluate node lÃ¡ báº±ng NN
       â””â”€ Value Network (value_network.py)
           â””â”€ Input: board_state.py (12Ã—8Ã—8)
```

## ğŸ® VÃ­ dá»¥ workflow

**BÆ°á»›c 1: Train láº§n Ä‘áº§u**
```bash
python main.py train
```

**BÆ°á»›c 2: ChÆ¡i game**
```bash
python main.py play --depth 3
```

**BÆ°á»›c 3: Táº¡o more data Ä‘á»ƒ train láº¡i**
```bash
python main.py selfplay --num-games 100 --white-mode minimax --black-mode minimax --save-data more_data.npz
```

**BÆ°á»›c 4: Train thÃªm (fine-tune)**
```bash
python main.py train --output-dir ./models
```

## ğŸ”‘ Key concepts

### Board Representation
- 12 planes: P,N,B,R,Q,K (white) + p,n,b,r,q,k (black)
- Má»—i plane 8Ã—8 binary matrix
- Efficient cho NN processing

### Minimax + NN Integration
- Minimax tÃ¬m kiáº¿m (vá»›i Alpha-Beta)
- NN khÃ´ng sinh nÆ°á»›c Ä‘i, chá»‰ Ä‘Ã¡nh giÃ¡
- Káº¿t há»£p = AI cÃ³ "suy tÆ°" + "ká»¹ nÄƒng"

### Self-play Learning
- Game 1: Random vs Random â†’ há»c cÆ¡ báº£n
- Game N: Minimax vs Minimax â†’ há»c chiáº¿n lÆ°á»£c nÃ¢ng cao
- Gradual improvement through iterations

## ğŸ“ˆ Training progression

```
Epoch 1   : loss=0.8523  val_loss=0.8412  (random knowledge)
Epoch 10  : loss=0.4231  val_loss=0.4189  (learning basic tactics)
Epoch 50  : loss=0.1234  val_loss=0.1289  (understanding positions)
Epoch 100 : loss=0.0234  val_loss=0.0245  (refined evaluation)
```

## ğŸ› ï¸ Debugging & Tricks

### Inspect position
```python
from board_state import BoardState
board_tensor = BoardState.board_to_tensor(board)
print(board_tensor.shape)  # (12, 8, 8)
```

### Test network
```python
from value_network import ValueNetwork
network = ValueNetwork()
value = network.evaluate_position(board_tensor)  # -1 to 1
```

### Check AI move
```python
engine = MinimaxEngine(network, max_depth=4)
move, score = engine.get_best_move_with_score(board)
print(f"Move: {move}, Score: {score:.4f}")
```

## ğŸ¯ Future improvements

- [ ] Add policy head (predict move distribution)
- [ ] Implement transposition tables
- [ ] Add opening book
- [ ] Iterative deepening
- [ ] More sophisticated evaluation (pawn structure, piece safety)
- [ ] NNUE architecture for faster eval
- [ ] Monte Carlo Tree Search (MCTS)

## ğŸ“š References

- **Minimax**: Classic game theory algorithm
- **Alpha-Beta Pruning**: Optimization for game trees
- **Self-play**: AlphaGo methodology
- **Value Networks**: Neural network as evaluator
- **Python-Chess**: Chess logic library
- **PyTorch**: Deep learning framework
- **Pygame**: Game graphics

## ğŸ“ Learning outcomes

âœ… Implement custom state representation
âœ… Build neural network for position evaluation
âœ… Implement minimax with pruning
âœ… Generate training data through self-play
âœ… Train model and validate
âœ… Integrate NN with game algorithm
âœ… Create interactive GUI
âœ… Full end-to-end AI system
