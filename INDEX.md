"""
AI CHESS SYSTEM - COMPLETE IMPLEMENTATION
Minimax + Neural Network + Self-play + GUI
"""

# =============================================================================
# ğŸ“ PROJECT STRUCTURE
# =============================================================================

AI Chess System/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                 â† START HERE! Project overview & usage
â”œâ”€â”€ ğŸ“„ QUICKSTART.md             â† Quick commands to get started
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md           â† Detailed technical architecture
â”œâ”€â”€ ğŸ“„ EXAMPLES.py               â† Code examples for each component
â”‚
â”œâ”€â”€ ğŸ”· BÆ¯á»šC 1: STATE REPRESENTATION
â”‚   â””â”€â”€ ğŸ“„ board_state.py        (12x8x8 tensor representation)
â”‚
â”œâ”€â”€ ğŸ”· BÆ¯á»šC 2: SELF-PLAY DATA GENERATION
â”‚   â””â”€â”€ ğŸ“„ self_play.py          (AI plays against itself)
â”‚
â”œâ”€â”€ ğŸ”· BÆ¯á»šC 3: NEURAL NETWORK DESIGN
â”‚   â””â”€â”€ ğŸ“„ value_network.py      (ValueNetwork, 768â†’128â†’64â†’1)
â”‚
â”œâ”€â”€ ğŸ”· BÆ¯á»šC 4: TRAINING LOOP
â”‚   â””â”€â”€ ğŸ“„ train.py              (PyTorch training with MSE + Adam)
â”‚
â”œâ”€â”€ ğŸ”· BÆ¯á»šC 5: MINIMAX + NN INTEGRATION
â”‚   â””â”€â”€ ğŸ“„ minimax_engine.py     (Minimax + Alpha-Beta + NN evaluation)
â”‚
â”œâ”€â”€ ğŸ”· BÆ¯á»šC 6: INTERACTIVE GUI
â”‚   â””â”€â”€ ğŸ“„ gui.py                (Pygame GUI for playing)
â”‚
â”œâ”€â”€ ğŸ® MAIN ENTRY POINT
â”‚   â””â”€â”€ ğŸ“„ main.py               (CLI with subcommands)
â”‚
â””â”€â”€ ğŸ“¦ DEPENDENCIES
    â””â”€â”€ ğŸ“„ requirements.txt       (Python packages)


# =============================================================================
# ğŸ¯ QUICK START
# =============================================================================

1. Install dependencies:
   $ pip install -r requirements.txt

2. Train model:
   $ python main.py train

3. Play game:
   $ python main.py play

4. Analyze position:
   $ python main.py analyze


# =============================================================================
# ğŸ“Š FILE DESCRIPTIONS
# =============================================================================

ğŸ”· BÆ¯á»šC 1: board_state.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Convert chess board â†” tensor (12Ã—8Ã—8)

Key functions:
  â€¢ board_to_tensor(board) â†’ numpy array
    - Input: chess.Board object
    - Output: (12, 8, 8) float32 tensor
    - Planes 0-5: White pieces (P,N,B,R,Q,K)
    - Planes 6-11: Black pieces (p,n,b,r,q,k)

  â€¢ tensor_to_board(tensor) â†’ chess.Board
    - Inverse transformation

  â€¢ get_game_result(board) â†’ float
    - Returns 1.0, 0.0, -1.0 for W/D/L

  â€¢ get_legal_moves_tensor(board) â†’ numpy array
    - Mask of legal move destinations

Example:
  tensor = BoardState.board_to_tensor(board)  # (12, 8, 8)
  board = BoardState.tensor_to_board(tensor)  # chess.Board


ğŸ”· BÆ¯á»šC 2: self_play.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Generate training data through self-play

Key classes:
  â€¢ SelfPlayGame
    - One game between two engines
    - Records all states and final result
    - play() â†’ (result, reason)

  â€¢ SelfPlayManager
    - Multiple games management
    - play_games(num_games, white_mode, black_mode)
    - get_training_data_batch()
    - save_training_data(), load_training_data()

Example:
  manager = SelfPlayManager()
  stats = manager.play_games(num_games=20, 
                             white_mode='random',
                             black_mode='random')
  board_tensors, labels = manager.get_all_data()


ğŸ”· BÆ¯á»šC 3: value_network.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Neural network for position evaluation

Key classes:
  â€¢ ValueNetwork
    Architecture:
      Input (768) â†’ Dense(128) + ReLU + Dropout
                  â†’ Dense(64) + ReLU + Dropout
                  â†’ Dense(1) + Tanh
      Output: value âˆˆ [-1, 1]

    Methods:
      â€¢ forward(x) â†’ predictions
      â€¢ evaluate_position(board_tensor) â†’ float
      â€¢ evaluate_positions_batch(batch) â†’ numpy array

Example:
  network = ValueNetwork(hidden_size=128)
  value = network.evaluate_position(board_tensor)  # â‰ˆ0.45


ğŸ”· BÆ¯á»šC 4: train.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Train value network with PyTorch

Key classes:
  â€¢ ChessTrainer
    - train_epoch(dataloader) â†’ loss
    - validate(dataloader) â†’ loss
    - train(train_data, val_data, epochs)
    - save_model(), load_model()
    - plot_loss()

  â€¢ full_training_pipeline()
    - End-to-end: self-play â†’ train â†’ save

Example:
  trainer = ChessTrainer(network, learning_rate=0.001)
  result = trainer.train(train_data, val_data, epochs=100)


ğŸ”· BÆ¯á»šC 5: minimax_engine.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Minimax + Alpha-Beta + NN evaluation

Key classes:
  â€¢ MinimaxEngine
    - Minimax with alpha-beta pruning
    - Uses NN for terminal node evaluation
    - max_depth: search depth limit

    Methods:
      â€¢ get_best_move(board) â†’ Move
      â€¢ get_best_move_with_score(board) â†’ (Move, float)
      â€¢ minimax(board, depth, maximizing, Î±, Î²) â†’ (value, move)

  â€¢ RandomEngine
    - Random move selector

Example:
  engine = MinimaxEngine(network, max_depth=3)
  move, score = engine.get_best_move_with_score(board)


ğŸ”· BÆ¯á»šC 6: gui.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Pygame GUI for interactive gameplay

Key classes:
  â€¢ ChessGUI
    - Draw 8Ã—8 board
    - Draw pieces (unicode symbols)
    - Handle mouse clicks
    - AI automatic moves
    - Game status display

    Methods:
      â€¢ run() â†’ main game loop
      â€¢ handle_click(pos, button)
      â€¢ ai_move()
      â€¢ reset()

Example:
  gui = create_gui_with_engine(model_path, ai_color=BLACK)
  gui.run()


ğŸ”· main.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Command-line interface entry point

Subcommands:
  â€¢ train
    - Full training pipeline
    - Usage: python main.py train --output-dir ./models

  â€¢ play
    - Interactive gameplay
    - Usage: python main.py play --depth 3 --player-color white

  â€¢ selfplay
    - Generate self-play data
    - Usage: python main.py selfplay --num-games 50 --white-mode minimax --save-data data.npz

  â€¢ analyze
    - Analyze position
    - Usage: python main.py analyze --depth 4 --fen "..."


# =============================================================================
# ğŸ”„ WORKFLOW EXAMPLES
# =============================================================================

Workflow 1: FIRST TIME SETUP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. pip install -r requirements.txt
2. python main.py train
   (Creates self-play data + trains network)
3. python main.py play
   (Play vs AI)


Workflow 2: CONTINUOUS IMPROVEMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. python main.py selfplay --num-games 100 \
     --white-mode minimax --black-mode minimax \
     --depth 3 --save-data data1.npz

2. python main.py train

3. python main.py selfplay --num-games 100 \
     --white-mode minimax --black-mode minimax \
     --depth 3 --save-data data2.npz
   
4. (Repeat 2-3)


Workflow 3: ANALYSIS ONLY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. python main.py analyze --depth 5
   (Analyze starting position)

2. python main.py analyze --depth 4 \
     --fen "r1bqkb1r/pppp1ppp/2n2n2/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 0 1"
   (Analyze specific position)


# =============================================================================
# ğŸ’¾ DATA FORMATS
# =============================================================================

Board State:
  Type: numpy.ndarray
  Shape: (12, 8, 8)
  dtype: float32
  Range: [0, 1] (0=empty, 1=piece exists)

Label:
  Type: float32
  Range: [-1, 0, 1]
  -1 = Black wins (bad for White)
   0 = Draw
  +1 = White wins (good for White)

Training Data (.npz file):
  states: (N, 12, 8, 8) - board states
  labels: (N,) - game outcomes

Model (.pth file):
  PyTorch state_dict
  Loadable via: network.load_state_dict(...)


# =============================================================================
# ğŸ® GAMEPLAY CONTROLS
# =============================================================================

While playing:
  â€¢ Left Click: Select piece or move to square
  â€¢ Right Click: Undo last move
  â€¢ R: Reset game
  â€¢ Q: Quit

Display shows:
  â€¢ 8Ã—8 board with pieces
  â€¢ Highlighted selected square
  â€¢ Blue dots for legal moves
  â€¢ Right panel with:
    - Current player (Tráº¯ng/Äen)
    - Move history
    - Controls


# =============================================================================
# ğŸ“ˆ EXPECTED PROGRESSION
# =============================================================================

Training Loss:
  Epoch 1:     loss â‰ˆ 0.85  (random)
  Epoch 10:    loss â‰ˆ 0.45  (learning)
  Epoch 50:    loss â‰ˆ 0.15  (progress)
  Epoch 100:   loss â‰ˆ 0.02  (convergence)

Game Strength (vs Random):
  Untrained NN:     ~50% win
  After 10 epochs:  ~60% win
  After 50 epochs:  ~75% win
  After 100 epochs: ~85% win


# =============================================================================
# ğŸ” DEBUGGING TIPS
# =============================================================================

Check board state:
  from board_state import BoardState
  tensor = BoardState.board_to_tensor(board)
  print(tensor.shape)  # Should be (12, 8, 8)

Check network output:
  value = network.evaluate_position(tensor)
  print(f"Value: {value:.4f}")  # Should be in [-1, 1]

Check minimax search:
  move, score = engine.get_best_move_with_score(board)
  print(f"Nodes: {engine.nodes_evaluated}")

Check training:
  print(f"Train loss: {trainer.train_losses}")
  print(f"Val loss: {trainer.val_losses}")

Inspect game:
  print(board)  # ASCII board
  print(board.fen())  # FEN notation
  print(board.move_stack)  # Move history


# =============================================================================
# ğŸ“š KEY CONCEPTS
# =============================================================================

State Representation:
  âœ“ 12 planes (6 white pieces + 6 black pieces)
  âœ“ 8Ã—8 binary matrix per plane
  âœ“ Efficient for neural network processing

Self-play:
  âœ“ AI plays against itself
  âœ“ All states get same outcome label
  âœ“ Creates diverse training data

Value Network:
  âœ“ Takes board state as input
  âœ“ Outputs evaluation score [-1, 1]
  âœ“ Not a move generator (policy)

Minimax:
  âœ“ Exhaustive game tree search
  âœ“ Max/Min layers for W/B alternation
  âœ“ Alpha-Beta pruning removes ~90% nodes

Alpha-Beta Pruning:
  âœ“ Optimization of minimax
  âœ“ Alpha: best value for maximizer
  âœ“ Beta: best value for minimizer
  âœ“ Cutoff when Î± â‰¥ Î²

Integration:
  âœ“ Minimax finds moves (tactical)
  âœ“ NN evaluates positions (strategic)
  âœ“ Combination = strong AI


# =============================================================================
# ğŸ“ NOTES & CAVEATS
# =============================================================================

â€¢ First run will be slow (untrained network)
  â†’ Network will improve with training iterations

â€¢ GPU speedup is optional
  â†’ CPU mode works fine for depth â‰¤ 3

â€¢ Board representation is not compact
  â†’ Could use bitboards, but clarity is prioritized

â€¢ Self-play data has winner bias
  â†’ All states in game get same label
  â†’ Better methods exist (q-learning, etc.)

â€¢ No opening book or endgame tables
  â†’ Could significantly improve opening/ending play

â€¢ Alpha-Beta pruning effectiveness varies
  â†’ Depends on move ordering
  â†’ Could be improved with killer moves


# =============================================================================
# ğŸš€ NEXT STEPS
# =============================================================================

1. Try all the examples in EXAMPLES.py
2. Run QUICKSTART.md commands
3. Read ARCHITECTURE.md for deep dive
4. Experiment with different depths/modes
5. Generate larger datasets and retrain
6. Modify network architecture and tune hyperparameters
7. Implement advanced techniques (policy head, MCTS, etc.)


# =============================================================================
# ğŸ† PROJECT SUMMARY
# =============================================================================

âœ… BÆ¯á»šC 1: State Representation (board_state.py)
   - Converts chess board to/from 12Ã—8Ã—8 tensor
   - Efficient neural network input format

âœ… BÆ¯á»šC 2: Self-Play (self_play.py)
   - AI plays against itself repeatedly
   - Generates labeled training data

âœ… BÆ¯á»šC 3: Neural Network (value_network.py)
   - 3-layer fully connected network
   - Evaluates position value [-1, 1]

âœ… BÆ¯á»šC 4: Training (train.py)
   - PyTorch training loop with validation
   - MSE loss, Adam optimizer, early stopping

âœ… BÆ¯á»šC 5: Minimax Integration (minimax_engine.py)
   - Minimax with alpha-beta pruning
   - NN evaluates leaf nodes

âœ… BÆ¯á»šC 6: GUI (gui.py)
   - Interactive Pygame interface
   - Click-to-move gameplay

âœ… Entry Point (main.py)
   - CLI with 4 subcommands
   - Easy one-line usage

âœ… Documentation
   - README.md: Overview
   - QUICKSTART.md: Commands
   - ARCHITECTURE.md: Technical details
   - EXAMPLES.py: Code samples


# =============================================================================
# ğŸ‰ READY TO USE!
# =============================================================================

Start here:
  1. python main.py train          (creates model)
  2. python main.py play           (play vs AI)
  3. python main.py analyze        (analyze positions)

Questions? Check:
  â€¢ README.md for overview
  â€¢ QUICKSTART.md for commands  
  â€¢ ARCHITECTURE.md for concepts
  â€¢ EXAMPLES.py for code samples

Enjoy your AI Chess! ğŸ
"""
