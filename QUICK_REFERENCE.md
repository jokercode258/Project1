"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    AI CHESS - QUICK REFERENCE CARD
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ COMMAND REFERENCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

INSTALLATION:
  pip install -r requirements.txt
  python validate.py

TRAINING:
  python main.py train                          # Full pipeline
  python main.py train --output-dir ./models    # Custom output

PLAYING:
  python main.py play                           # Default settings
  python main.py play --depth 3                 # Set depth
  python main.py play --player-color white      # Choose color
  python main.py play --model ./models/chess_value_network.pth  # Load model

SELF-PLAY:
  python main.py selfplay --num-games 50       # Generate data
  python main.py selfplay --white-mode minimax --black-mode minimax --depth 3
  python main.py selfplay --save-data data.npz  # Save training data

ANALYSIS:
  python main.py analyze                        # Analyze start position
  python main.py analyze --depth 4              # Custom depth
  python main.py analyze --fen "..."            # Analyze FEN position


ğŸ“¦ FILE MAP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CORE MODULES:
  board_state.py        â†’ 12Ã—8Ã—8 tensor representation
  value_network.py      â†’ Neural network (768â†’128â†’64â†’1)
  minimax_engine.py     â†’ Minimax + Alpha-Beta + NN evaluation
  self_play.py          â†’ Self-play games & data generation
  train.py              â†’ PyTorch training loop
  gui.py                â†’ Pygame interactive GUI
  main.py               â†’ CLI entry point

DOCUMENTATION:
  README.md             â†’ Start here! Overview & quick start
  QUICKSTART.md         â†’ Commands & workflows
  ARCHITECTURE.md       â†’ Technical deep dive (600+ lines)
  EXAMPLES.py           â†’ Code examples for each module
  INDEX.md              â†’ Complete index & reference
  IMPLEMENTATION_SUMMARY.md â†’ Project summary

CONFIG:
  requirements.txt      â†’ Dependencies
  validate.py           â†’ System validation


ğŸ¯ KEY CLASSES & FUNCTIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BoardState (board_state.py):
  board_to_tensor(board) â†’ numpy (12, 8, 8)
  tensor_to_board(tensor) â†’ chess.Board
  get_legal_moves_tensor(board) â†’ mask
  get_game_result(board) â†’ float [-1, 0, 1]

ValueNetwork (value_network.py):
  __init__(hidden_size=128, dropout=0.3)
  forward(x) â†’ predictions
  evaluate_position(board_tensor) â†’ float
  evaluate_positions_batch(batch) â†’ numpy array

MinimaxEngine (minimax_engine.py):
  __init__(network, device, max_depth=3)
  get_best_move(board) â†’ Move
  get_best_move_with_score(board) â†’ (Move, float)
  minimax(board, depth, maximizing, Î±, Î²) â†’ (value, move)

SelfPlayGame (self_play.py):
  play() â†’ (result, reason)
  get_training_data() â†’ [(state, label), ...]

SelfPlayManager (self_play.py):
  play_games(num_games, white_mode, black_mode) â†’ stats
  get_all_data() â†’ (board_tensors, labels)
  save_training_data(filepath)
  load_training_data(filepath)

ChessTrainer (train.py):
  train_epoch(dataloader) â†’ loss
  validate(dataloader) â†’ loss
  train(train_data, val_data, epochs) â†’ result
  save_model(filepath)
  load_model(filepath)
  plot_loss(save_path)

ChessGUI (gui.py):
  run() â†’ game loop
  handle_click(pos, button)
  ai_move()
  reset()


ğŸ“Š HYPERPARAMETERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Network:
  hidden_size: 128          # First hidden layer
  dropout: 0.3              # Regularization
  output_range: [-1, 1]     # Tanh activation

Training:
  learning_rate: 0.001      # Adam optimizer
  weight_decay: 1e-5        # L2 regularization
  batch_size: 32
  epochs: 100
  early_stopping: 10        # Patience

Minimax:
  max_depth: 3              # Search depth (1-5 typical)
  alpha_init: -inf
  beta_init: +inf

Self-play:
  num_games: 20-100
  max_moves_per_game: 50-100


ğŸ§© DATA FORMATS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Board State (tensor):
  Shape: (12, 8, 8)
  dtype: float32
  Range: [0, 1]
  Planes: 0-5 white, 6-11 black

Label:
  Type: float32
  Value: -1.0 (black wins), 0.0 (draw), 1.0 (white wins)

Training Data (.npz):
  states: (N, 12, 8, 8)
  labels: (N,)

Model (.pth):
  PyTorch state_dict
  Loadable via: load_state_dict(torch.load(...))


âš¡ PERFORMANCE TIPS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Make AI Stronger:
  â€¢ Increase depth: --depth 5 (but slower)
  â€¢ Train longer: epochs=200 (needs more data)
  â€¢ Use minimax: --white-mode minimax (stronger than random)

Make AI Faster:
  â€¢ Decrease depth: --depth 2 (weaker)
  â€¢ Use GPU: automatic if available
  â€¢ Reduce batch size: batch_size=16

Generate Better Data:
  â€¢ Use minimax vs minimax (best data)
  â€¢ Increase num_games: 100+ games
  â€¢ Vary depths: mix different depths

Improve Network:
  â€¢ Increase hidden size: 256 or 512
  â€¢ Add more layers: add Dense(128) â†’ Dense(64)
  â€¢ Longer training: 200+ epochs
  â€¢ More data: 10k+ self-play games


ğŸ® GAMEPLAY TIPS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For beginners:
  â€¢ Start with depth 2
  â€¢ Play as white (easier)
  â€¢ Study AI's moves

For intermediate:
  â€¢ Use depth 3-4
  â€¢ Play both colors
  â€¢ Use analysis mode

For advanced:
  â€¢ High depth (5+) takes time
  â€¢ Batch training: 100+ games per iteration
  â€¢ Analyze lost positions


ğŸ› TROUBLESHOOTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Problem: "ModuleNotFoundError"
Solution: pip install -r requirements.txt

Problem: "GPU out of memory"
Solution: Reduce batch_size or use device='cpu'

Problem: "AI takes too long"
Solution: Reduce depth or use smaller model

Problem: "Model doesn't improve"
Solution: Generate more self-play data first

Problem: "Pygame won't display"
Solution: Check X11 settings or use headless

Problem: "Board state shape error"
Solution: Verify input is chess.Board object

Problem: "Training loss stays high"
Solution: Check data quality, increase learning_rate


ğŸ“ˆ EXPECTED RESULTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

After 1 training run (20 self-play games):
  â€¢ Loss: 0.3-0.5
  â€¢ Time: 5-10 minutes
  â€¢ Strength: Weak but playable

After 100 epochs training:
  â€¢ Loss: 0.02-0.05
  â€¢ Win rate vs random: 70-80%
  â€¢ Strength: Decent
  â€¢ Time per move: 1-2 seconds (depth 3)

After 1000+ self-play games + training:
  â€¢ Loss: 0.01-0.02
  â€¢ Win rate vs random: 90%+
  â€¢ Strength: Strong
  â€¢ Can beat casual players


ğŸ”— INTEGRATION POINTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Custom game board:
  â€¢ Replace chess.Board with custom implementation
  â€¢ Keep BoardState interface same

Different network architecture:
  â€¢ Modify ValueNetwork.__init__()
  â€¢ Ensure output shape (batch, 1)

Alternative engine:
  â€¢ Implement same interface as MinimaxEngine
  â€¢ get_best_move(board) â†’ Move

Different GUI:
  â€¢ Keep same interface
  â€¢ Modify draw methods

Custom training data:
  â€¢ Load with self_play.SelfPlayManager.load_training_data()
  â€¢ Ensure shape (N, 12, 8, 8) and labels (N,)


ğŸ“š KEY INSIGHTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Minimax finds best moves, NN evaluates positions
2. Alpha-Beta pruning removes ~90% of nodes
3. Self-play creates unlimited training data
4. Early stopping prevents overfitting
5. Larger depths = stronger but slower
6. 12 planes better than single board matrix
7. Tanh output [-1, 1] matches game outcomes
8. GUI runs independently of training
9. All components are modular & replaceable
10. System scales from 1 to 1000+ games


ğŸ“ LEARNING RESOURCES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

In this project:
  â€¢ ARCHITECTURE.md: Technical foundations
  â€¢ EXAMPLES.py: Code usage patterns
  â€¢ Source code: Implementation details

External:
  â€¢ python-chess docs: Board & move handling
  â€¢ PyTorch docs: Neural networks
  â€¢ Pygame docs: GUI development
  â€¢ Wikipedia: Minimax, Alpha-Beta
  â€¢ Books: "AI: A Modern Approach"


âœ… VALIDATION CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Before using:
  â˜ pip install -r requirements.txt
  â˜ python validate.py (all checks pass)
  â˜ Read README.md
  â˜ Run one example: python main.py train

For development:
  â˜ Understand all 6 steps
  â˜ Review ARCHITECTURE.md
  â˜ Test each module individually
  â˜ Read source code comments

For deployment:
  â˜ Train model adequately (100+ epochs)
  â˜ Validate with test games
  â˜ Document any customizations
  â˜ Version control setup


ğŸš€ GETTING STARTED NOW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â±ï¸ 5 minutes:
   1. pip install -r requirements.txt
   2. python validate.py
   3. Read README.md

â±ï¸ 15 minutes:
   4. python main.py train
   5. python main.py play

â±ï¸ 1 hour:
   6. Explore source code
   7. Read ARCHITECTURE.md
   8. Try different parameters

â±ï¸ Full day:
   9. Deep dive into implementation
   10. Modify & experiment
   11. Generate custom data
   12. Train stronger model


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    Ready to dive in? Start here:
                    python main.py train
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
